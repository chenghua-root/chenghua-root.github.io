---  
layout: post  
title:  "Linux内核管理与进程调度"  
date:  2020-09-19 16:48:00 +0530  
---  
  
<style>  
.tablelines table, .tablelines td, .tablelines th {  
  border: 1px solid black;  
  }  
</style>  
  
## 背景  
工作初期一直使用同步编程模式。包括协程(Go)，多线程并发，同步IO，事件驱动IO，条件等待唤醒等。对外部的响应都依赖内核的中断响应和处理。  
  
现在开始使用单线程Polling+用户态异步IO的编程模式。采用单线程+用户态主要是为了高性能，即减少线程切换，内核栈调用，内核中断处理等开销。  
  
为了探寻多线程+内核栈编程的具体开销情况，学习进程管理、中断机制、时钟模型、同步原理等知识点并记录在此篇文章中。  
  
**本文为作者的学习笔记，不是综述性文章，仅做参考。**  
  
## 概述  
进程（线程）管理、虚拟内存和文件系统是单机系统最重要的几个底层原理。  
本文主要讲解Linux内核进程相关的内容，包括进程管理与调度，用户态与内核态，中断服务，进程同步，时钟机制等。  
  
## 内核  
**内核组成：**  
- 负责响应中断的中断服务程序  
- 负责管理多个进程从而分享处理器时间的调度程序  
- 负责管理进程地址空间的内存管理程序  
- 网络、进程间通信等系统服务程序  
  
**内核空间：**  
Linux内核把虚拟地址空间划分为两部分：用户地址空间和内核地址空间。  
32位系统(3G/1G划分)：  
- 用户空间3G：0x0000,0000 - 0xBFFF,FFFF  
- 内核空间1G：0xC000,0000 - 0xFFFF,FFFF  
  
64位系统：  
- 用户空间128T：0x0000,0000,0000,0000 - 0x0000,7FFF,FFFF,FFFF(高16位与第48位都为0)  
- 内核空间128T：0xFFFF,8000,0000,0000 - 0xFFFF,FFFF,FFFF,FFFF(高16位与第48位都为1)  
  
**系统态&内核空间：**  
- 系统态：拥有【受保护的内存空间和访问硬件设备】的所有权限。  
- 内核空间：系统态和被包含起来的内存空间，统称为内核空间。  
  
内核运行时，系统以内核态进入内核空间执行。  
执行普通用户程序时，系统将以用户态进入以用户空间执行。它们只能看到允许它们使用的部分系统资源，并且只能使用某些特定的系统功能，不能直接访问硬件，也不能访问内核划给别人的物理内存。  
  
**系统调用**  
在系统中运行的应用程序通过系统调用来与内核通信。  
应用程序被称为通过系统调用在内核空间运行，而内核被称为运行于进程上下文中。注意：是在内核空间运行，只是运行在进程上下文中。  
  
**中断**  
- 内核负责管理硬件设备。几乎所有的体系结构，包括全部Linux支持的体系结构，都提供了中断机制。  
- 当硬件设备想和系统通信的时候，它首先要发一个异步的中断信号去打断处理器的执行，继而打断内核的执行。  
- 中断通常对应一个中断号，内核通过这个中断号查找相应的中断服务程序。并调用这个程序响应和处理中断。  
- 为了保证中断的同步执行，内核可以停止终止--停止所有的中断或者部分中断。  
- Linux的中断服务程序在一个与所有进程都无关的、专门的中断上下文中运行。  
  
处理器在任何指定时间点上的活动必然处于下列三者之一：  
- 运行于用户空间，执行用户进程。  
- 运行于内核空间，处于进程上下文，代表某个特定的进程执行。  
- 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。  
以上所列包含了所有情况。例如：当CPU空闲时，内核就运行一个空进程，处于进程上下文，但运行于内核空间。  
  
**单内核vs微内核**  
Linux是单内核，作为一个整体运行在一个单独的地址空间。模块化、多线程以及内核本身可调度的操作系统。  
  
### 内核开发特点  
**没有内存保护机制**  
- 用户程序进行非法的内存访问，内核会发现这个错误，发送SIGSEGV，如访问内核空间地址  
- 用户程序受虚拟内存管理，不会访问到其它进程的物理内存  
  
**容积小而固定的栈**  
- 用户栈空间大小一般为8MB，支持动态增长  
- 内核栈大小为8kB或16kB  
  
**同步和并发**  
- 内核很容易产生竞争条件。内核许多特性都要求能够并发地访问共享数据  
- Linux是抢占多任务操作系统  
- Linux内核支持对称多处理器系统（SMP）  
-- 如果没有适当的保护，同时在多个处理器上的内核代码可能会同时访问同一个资源  
  
- **中断是异步到来的，完全不顾及当前正在执行的代码**  
-- 中断完全可能在代码访问资源的时候到来，这样，中断处理程序就有可能访问同一资源  
- 通常解决竞争的办法是自旋锁和信号量  
  
## 进程管理  
### 进程&线程  
线程: 进程活动的对象。每个线程都拥有一个独立的程序计算器、栈和一组寄存器。Linux中，线程和进程并不特别区分，线程只不过是一种特殊的进程罢了。  
  
现代操作系统中，进程提供两种虚拟机制：虚拟处理器和虚拟内存。  
同一个进程中的线程可以共享虚拟内存，但每个线程都拥有各自的虚拟处理器。  
  
### 进程描述符及任务结构  
内核把进程的列表存放在任务队列的双向循环链表中。链表中的每一项都是类型为task_struct、称为进程描述符的结构，包含了一个具体进程的所有信息。  
task_struct在32位机器中大约有1.7KB。进程描述符中的数据能完整的描述一个正在执行的程序：它打开的文件，进程的地址空间，挂起的信号，进程的状态等其他更多的信息。  
  
**进程描述符**  
- 早期存放在内核栈的尾端  
- 现在使用slab分配器动态生成，所以在栈底创建一个新的结构task_info，task_info->task_struct指向新创建的task_struct  
- 每个线程都有一个task_struct，非主线程没有独立的虚拟地址空间  
  
**内核栈和task_info的存储**  
- 内核栈: 每个线程都有一个内核栈stack  
- thread_info: 保存了特定体系结构的汇编代码段需要访问的那部分进程的数据  
- 内核栈和thread_info统一存储在一个联合体中union thread_unio  
  
![task_info](https://chenghua-root.github.io/images/linux-kernel-thread-info.png)  
  
 - thread_info和内核栈虽然共用了thread_union结构, 但是thread_info大小固定, 存储在联合体的开始部分, 而内核栈由高地址向低地址扩展, 当内核栈的栈顶到达thread_info的存储空间时, 则会发生栈溢出  
 - 系统的current指针指向了当前运行进程的thread_union(或者thread_info)的地址  
 - 进程task_struct中的stack指针指向了进程的thread_union(或者thread_info)的地址  
 - 当进程从用户态切换到内核态时，内核栈总是空的（esp指向栈顶）  
 - 参考：https://blog.csdn.net/gatieme/article/details/51577479  
  
**进程状态：**  
- TASK_RUNNING：可执行状态（存放在红黑树中）；或者正在执行，或者在运行队列中等待执行  
- TASK_INTERRUPTIBLE：被阻塞（睡眠态）(存放在等待队列);  -等待某些条件的达成，也会因为接收到信号(软中断，硬中断)而提前被唤醒  
-- 被信号唤醒属于伪唤醒，执行完中断后继续睡眠  
- TASK_UNINTERRUPTIBLE：被阻塞（睡眠态）(存放在等待队列); -等待某些条件的达成，接收到信号也不会被唤醒，只能由内核亲自唤醒（wake_up()）  
- TASK_TRACED：被其他进程跟踪的进程  
- TASK_STOPPED：进程停止执行  
  
![进程状态图](https://chenghua-root.github.io/images/linux-kernel-task-state.png)  
  
![进程队列图](https://chenghua-root.github.io/images/linux-kernel-ready-and-wait-queue.png)  
  
**内核线程：**  
- 内核线程和普通线程的区别在于内核线程没有独立的地址空间（mm指针被设置为NULL）  
- 只在内核空间运行，从来不切换到用户空间去  
- 和普通线程一样，可以被调度，也可以被抢占  
  
### 进程的终结  
**do_exit()进行进程终结**  
- 调用exit_mm()函数释放占用的mm_struct  
- 调用sem_exit()函数  
- 调用exit_files()和exit_fs()，分别递减文件描述符、文件系统数据的引用计数  
- 调用exit_notify()向父进程发送信号。把进程状态设置为EXIT_ZOMBIE  
- 最后调用schedule()切换到新的进程。因为处于EXIT_ZOMBIE状态的进程不会再被调度，所以这是进程执行的最后一段代码。do_exit()永不返回  
- 此时，与进程相关的所有资源都释放掉了，进程不可运行  
-- 占用的所有内存就是内核栈、thread_info结构和task_struct结构  
  
**删除进程描述符**  
- release_task()会被调用  
-- 从pidhash上删除该进程，同时也从任务列表中删除该进程  
-- 调用put_task_struct()释放进程内核栈和thread_info结构所占的页，并释放task_struct占用的slab内存  
  
**思考**  
通过task_struct对进程进行管理。通过内核栈对进程进行执行。  
当调度执行某个进程时（内核态），esp寄存器设置内核栈顶指针（task_struct->stack）。运行时为了高速获取task_struct，使用esp屏蔽底13位获取到栈底地址。栈底地址即存放了threa_info->task_struct。  
  
## 进程调度  
### 多任务  
Linux采用抢占式的调度。由调度程序来决定什么时候停止一个进程的运行。  
- 早期Go版本中协程采用非抢占式，除非自己停止或被阻塞，否则一直占用物理线程。  
  
### Linux的进程调度  
- O(1)调度算法  
- 反转楼梯最后期限调度算法，该算法吸取了队列理论，将公平调度的概念引入了Linux调度程序。在2.6中被称为完全公平调度算法，简称CFS  
  
### 调度策略  
I/O消耗型和处理器消耗型的进程  
- I/O消耗经常处于可运行状态，但通常都是运行较短时间  
- 处理器消耗型大多时间在执行代码上。因此，从系统响应速度考虑，不应该经常调度它们运行。对它们的调度策略往往是：降低调度频率，增加每次运行的时间。  
  
**调度策略考虑的点：**  
- 进程响应迅速  
- 最大系统利用率（高吞吐量），减少进程切换  
- Linux倾向于优先调度I/O消耗型进程  
  
**进程优先级**  
- 根据进程的价值和其对处理器时间的需求来对进程分级的想法。  
- 通常：优先级高的先运行，底的后运行  
- 相同优先级的进程按轮转方式进行调度  
  
**Linux采用两种不同的优先级范围**  
- nice值。-20~+19，越大的nice值意味着更低的优先级。  
-- Linux中，nice值代表时间片的比例  
- 实时优先级。0~99，越高的实时优先级数值意味着进程优先级越高。  
-- 任何实时进程的优先级都高于普通进程  
-- ps -eo state,uid,pid,ppid,rtprio,time,comm  
  
![进程队列图](https://chenghua-root.github.io/images/linux-kernel-priority.png)  
  
**时间片**  
- 时间片太长会导致系统对交互的响应表现欠佳  
- 时间片太短会增大进程切换带来的处理器耗时  
  
**唤醒的进程是否立刻投入执行**  
- 一个文字编辑程序 vs 一个视频编码程序  
  
### Linux调度算法  
**调度器类**  
- Linux调度器是以模块方式提供的，目的是允许不同类型的进程可以有针对性的选择调度算法  
- 调度器类包含多个调度器，每个调度器都有一个优先级，按照优先级顺序遍历调度类  
  
**CFS**  
- CFS在所有可运行进程总数基础上计算出一个进程应该运行多久，而不是依靠nice值来计算时间片  
- nice值在CFS中被作为进程获得的处理器运行比的权重：nice值越高的进程获得更低的处理器使用权重  
- 每个进程都按照其权重在全部可运行进程中所占比例的“时间片”来运行  
- CFS为每个进程获得的时间片设置了一个底线，称为最小粒度，默认为1ms  
  
### Linux调度的实现  
Linux调度主要关注如下四个部分：  
- 实时记账  
- 进程选择  
- 调度器入口  
- 睡眠和唤醒  
  
**时间记账**  
每个进程分配一个时间片。当每次系统时钟节拍器发生时，时间片都会被减少一个节拍周期。一个进程的时间片被减少到0时，它就会被另一个尚未减少到0的时间片可运行进程抢占。  
用vruntime表示当前进程的运行时间  
  
**挑选运行进程**  
所有进程以vruntime为值构建红黑树，每次挑选一个vruntime最小的进程。  
  
**睡眠和唤醒**  
当进程阻塞时，进程把自己标记成休眠状态，从可执行红黑树中移除，放入等待队列，然后调用schedule()选择和执行一个其他进程。  
唤醒过程相反：进程被设置为可执行状态，然后再从等待队列中移到可执行红黑树中。  
  
**唤醒**  
通过函数wake_up()唤醒指定队列上的所有进程。置为TASK_RUNNING状态。如果被唤醒的进程的优先级比当前的进程的优先级高，还要设置need_resched标志。  
  
**抢占和上下文切换**  
上下文切换，就是从一个可执行进程切换到另一个可执行进程。由context_switch()函数负责处理。它完成两项基本工作：  
- 调用switch_mm()，负责把虚拟内存从上一个进程映射切换到新进程中。  
- 调用switch_to()，负责上一个进程的处理器状态切换到新进程的处理器状态。  
-- 保存、恢复栈信息和寄存器信息，其它任何与体系结构相关的状态信息  
- 每个进程都包含一个need_resched标志，存放在thread_info中以加快访问  
  
**用户抢占**  
用户抢占在以下情况时产生：  
- 从系统调用返回用户空间时  
- 从中断处理程序返回用户空间时  
  
**内核抢占**  
如果没有持有锁，正在执行的代码就是可重新导入的，也就是可以抢占的  
内核抢占发生时间：  
- 中断处理程序正在执行，且返回内核空间之前  
- 内核代码再一次具有可抢占性的时候  
- 如果内核中的任务显式的调用schedule()  
- 如果内核中的任务阻塞  
  
**调度策略&&实时调度策略**  
普通的、非实时的调度策略是SCHED_NORMAL.  
两种实时调度策略是：SCHED_FIFO和SCHED_RR.  
  
SCHED_FIFO实现了一种简单的、先入先出的调度算法。处于可运行状态的SCHED_FIFO级的进程比如何SCHED_NORMAL的进程都先得到调度。  
SCHED_FIFO会一直运行下去，或者被更高优先级的SCHED_FIFO或SCHED_RR抢占。  
SCHED_RR是带有时间片的SCHED_FIFO。  
  
**放弃处理器时间**  
- 系统接口：sched_yield()  
- 内核函数：schedule()  
  
## 完全公平调度CFS(Completely Fair Scheduler)  
### CFS介绍  
- 设定一个调度周期（sched_latency_ns），目标是让每个进程在这个周期内至少有机会运行一次，换一种说法就是每个进程等待CPU的时间最长不超过这个调度周期；  
- 然后根据进程的数量，大家平分这个调度周期内的CPU使用权，由于进程的优先级即nice值不同，分割调度周期的时候要加权；  
- 每个进程的累计运行时间保存在自己的vruntime字段里，哪个进程的vruntime最小就获得本轮运行的权利。  
- 理解vruntime是理解cfs思想以及理解调度本身的核心  
  
**如何确定调度周期?**  
- 查看系统调度周期cat /proc/sys/kernel/sched_latency_ns 24000000 // 24毫秒  
- 为了避免线程频繁切换，设定了一个线程占用CPU的最小时间值。  
- cat /proc/sys/kernel/sched_min_granularity_ns 3000000 // 3毫秒  
- 当线程数超过24/3=8时，调度周期会会设定为sched_min_granularity_ns * 线程个数。  
  
**如何理解线程优先级？**  
- Linux 采用两个单独的优先级范围，一个用于实时任务，另一个用于普通任务。  
- 实时任务分配的静态优先级为 0〜99，而普通任务分配的优先级为100〜139(对应nice值-20 ~ 19)。  
|--------------------------------------|----------------|  
0------------------------------------99 100------------139  
<------------------------------------------------------->  
高优先级----------------------------------------------低优先级  
- 实时任务和普通任务的调度分开管理。实时任务总是优先调度。  
  
优先级的实质是不同优先级线程对应不同的权重，**在一个调度周期内分配的运行时间不一样。**  
如下列出了普通优先级对应的权重:  
  ![task_info](https://chenghua-root.github.io/images/linux-kernel-nice.png)  
  
### vruntime  
**vruntime计算**  
- 执行时间：delta_exec = (now – curr->exec_start)  
- 加权计算：delta_exec_weighted = calc_delta_fair(delta_exec, curr) = delta_exec * (NICE_0_LOAD / curr->load.weight)  
- 加权运行时间：vruntime += delta_exec_weighted  
- 综合上面公式：vruntime = (NICE_0_LOAD / curr->load.weight) * SUM(runtime)  
  
得到这个vruntime后，系统会计算每个线程的vruntime排序（使用红黑树），vruntime最小的线程会最早获得调度。一旦vruntime的次序发生变化，系统将尝试触发下一次调度。也就是说调度器尽可能的保证所有线程的vruntime都一致。而权重高(nice值底)的线程vruntime提升的慢，容易被优先调度。  
  
**vruntime查看**  
$cat /proc/${PID}/sched  
  
### CFS调度相关问题  
**新线程的vruntime初值是不是0啊？**  
假如新进程的vruntime初值为0的话，比老进程的值小很多，那么它在相当长的时间内都会保持抢占CPU的优势，老进程就要饿死了，这显然是不公平的。所以CFS是这样做的：每个CPU的运行队列cfs_rq都维护一个min_vruntime字段，记录该运行队列中所有进程的vruntime最小值，新进程的初始vruntime值就以它所在运行队列的min_vruntime为基础来设置，与老进程保持在合理的差距范围内。  
  
新进程的vruntime初值的设置与两个参数有关：  
sched_child_runs_first：规定fork之后让子进程先于父进程运行;  
sched_features的START_DEBIT位：规定新进程的第一次运行要有延迟。  
  
子进程在创建时，vruntime初值首先被设置为min_vruntime；然后，如果sched_features中设置了START_DEBIT位，vruntime会在min_vruntime的基础上再增大一些。设置完子进程的vruntime之后，检查sched_child_runs_first参数，如果为1的话，就比较父进程和子进程的vruntime，若是父进程的vruntime更小，就对换父、子进程的vruntime，这样就保证了子进程会在父进程之前运行。  
  
**休眠进程的vruntime一直保持不变吗？**  
如果休眠进程的 vruntime 保持不变，而其他运行进程的 vruntime 一直在推进，那么等到休眠进程终于唤醒的时候，它的vruntime比别人小很多，会使它获得长时间抢占CPU的优势，其他进程就要饿死了。这显然是另一种形式的不公平。CFS是这样做的：在休眠进程被唤醒时重新设置vruntime值，以min_vruntime值为基础，给予一定的补偿（减去一定值），但不能补偿太多。  
  
 **休眠进程在唤醒时会立刻抢占CPU吗？**  
这是由CFS的唤醒抢占 特性决定的，即sched_features的WAKEUP_PREEMPT位。  
  
由于休眠进程在唤醒时会获得vruntime的补偿，所以它在醒来的时候有能力抢占CPU是大概率事件，这也是CFS调度算法的本意，即保证交互式进程的响应速度，因为交互式进程等待用户输入会频繁休眠。除了交互式进程以外，主动休眠的进程同样也会在唤醒时获得补偿，例如通过调用sleep()、nanosleep()的方式，定时醒来完成特定任务，这类进程往往并不要求快速响应，但是CFS不会把它们与交互式进程区分开来，它们同样也会在每次唤醒时获得vruntime补偿，这有可能会导致其它更重要的应用进程被抢占，有损整体性能。  
  
可以关闭唤醒抢占。  
  
**sched_wakeup_granularity_ns**  
这个参数限定了一个唤醒进程要抢占当前进程之前必须满足的条件：只有当该唤醒进程的vruntime比当前进程的vruntime小、并且两者差距(vdiff)大于sched_wakeup_granularity_ns的情况下，才可以抢占，否则不可以。这个参数越大，发生唤醒抢占就越不容易。  
  
**进程占用的CPU时间片可以无穷小吗？**  
假设有两个进程，它们的vruntime初值都是一样的，第一个进程只要一运行，它的vruntime马上就比第二个进程更大了，那么它的CPU会立即被第二个进程抢占吗？答案是这样的：为了避免过于短暂的进程切换造成太大的消耗，CFS设定了进程占用CPU的最小时间值，sched_min_granularity_ns，正在CPU上运行的进程如果不足这个时间是不可以被调离CPU的。  
  
sched_min_granularity_ns发挥作用的另一个场景是，本文开门见山就讲过，CFS把调度周期sched_latency按照进程的数量平分，给每个进程平均分配CPU时间片（当然要按照nice值加权，为简化起见不再强调），但是如果进程数量太多的话，就会造成CPU时间片太小，如果小于sched_min_granularity_ns的话就以sched_min_granularity_ns为准；而调度周期也随之不再遵守sched_latency_ns，而是以 (sched_min_granularity_ns * 进程数量) 的乘积为准。  
  
**进程从一个CPU迁移到另一个CPU上的时候vruntime会不会变？**  
在多CPU的系统上，不同的CPU的负载不一样，有的CPU更忙一些，而每个CPU都有自己的运行队列，每个队列中的进程的vruntime也走得有快有慢，比如我们对比每个运行队列的min_vruntime值，都会有不同：  
  
CFS是这样做的：  
- 当进程从一个CPU的运行队列中出来 (dequeue_entity) 的时候，它的vruntime要减去队列的min_vruntime值；  
- 而当进程加入另一个CPU的运行队列 ( enqueue_entiry) 时，它的vruntime要加上该队列的min_vruntime值。  
- 这样，进程从一个CPU迁移到另一个CPU之后，vruntime保持相对公平。  
  
**vruntime会溢出吗**  
- vruntime使用uint64_t保持，递增，不会overflow  
  1 year = 365 x 24 x 60 x 60 x 1000 x 1000 x 1000 nanosec  
  ~= 3 x 10^16  
  ~= 2^55  
  2^64 / 2^55 = 2^9 ~= 500 year.  
  
**Linux下的调度器是可以有多个同时工作的吗，比如rt 和cfs一起同时工作？**  
调度器的本质工作是为CPU选择一个运行进程，调度时会“遍历”调度器（依次运行）直到选择到合适的可运行进程。因此，多个调度器同时存在，但非同时运行。具体分析如下。  
先梳理概念  
调度策略：即调度算法，一共6种，每个进程属于其中一种（task_struct->policy）  
- SCHED_RR和SCHED_FIFO和SCHED_DEADLINE：实时进程对应的不同调度策略  
- SCHED_NORMAL和SCHED_BATCH：普通的非实时进程对应的调度策略  
- SCHED_IDLE：系统空闲时调用idle进程对应的调度策略  
  
调度器类：每个进程属于其中一种（task_struct->sched_class）  
- stop_sched_class -> dl_sched_class -> rt_sched_class -> fair_sched_class -> idle_sched_class（优先级由高到底）  
- rt_sched_class->fair_sched_class->idle_sched_class(linux-2.6.34.7)  
- stop_sched_class->rt_sched_class->fair_sched_class->idle_sched_class(linux-3.10)  
  
调度器和调度策略存在映射关系：  
  
  | 调度器	| 调度策略 |  
  | -----            | -----                    |  
  | stop_sched_class |  无 |  
  | dl_sched_class   | SCHED_DEADLINE |  
  | rt_sched_class   | SCHED_FIFO、SCHED_RR |  
  | fair_sched_class | SCHED_NORMAL、SCHED_BATCH |  
  | idle_sched_class | SCHED_IDLE|  
{: .tablelines}  
  
再分析调度  
  
每个CPU都有一个struct rq对象，包含两个运行队列：  
  
- struct rt_rq：实时进程运行队列  
- struct cfs_rq：普通进程运行队列  
其中rt_sched_class使用rt_rq，fair_sched_class使用cfs_rq。其它调度器不需要队列。  
  
调度执行时机（什么时候执行调度）：  
- 一种是通过周期性的机制, 以固定的频率运行, 不时的检测是否有必要  
- 一种是直接的, 比如进程打算睡眠或出于其他原因放弃CPU  
调度执行：  
```  
static void __sched __schedule(void)  
{  
  ...  
  cpu = smp_processor_id();  
  rq = cpu_rq(cpu);  
  prev = rq->curr;  
  ...  
  put_prev_task(rq, prev);  
  next = pick_next_task(rq);  
  ...  
  if (likely(prev != next)) {  
    ...  
    rq->curr = next;  
    ...  
    context_switch(rq, prev, next);  
    ...  
  }  
  ...  
}  
```  
- 获取CPU  
- 获取CPU队列（后面调度器运行时会选择自己对应的运行队列，rt_rq或cfs_rq）  
- 获取当前进程  
- 把当前进程放入队列  
- 从队列选取新的运行进程  
- 如果新进程与当前进程不为同一进程，则执行上下文切换  
  
放回队列：  
```  
static void put_prev_task(struct rq *rq, struct task_struct *prev)  
{  
  ...  
  prev->sched_class->put_prev_task(rq, prev);  
}  
```  
- 使用对应的调度器把当前进程放入队列  
  
挑选下一个运行进程：  
```  
/*  
 * #define sched_class_highest (&rt_sched_class)   // linux-2.6.34.7   
 * #define sched_class_highest (&stop_sched_class) // linux-3.10  
 *   
 * idle_sched_class->next = null;  
 */  
#define for_each_class(class) \  
   for (class = sched_class_highest; class; class = class->next)  
  
static inline struct task_struct *pick_next_task(struct rq *rq)  
{  
  const struct sched_class *class;  
  struct task_struct *p;  
  ...  
  for_each_class(class) {  
    p = class->pick_next_task(rq);  
    if (p)  
      return p;  
  }  
}  
```  
-依次遍历调度器stop_sched_class->rt_sched_class->fair_sched_class->idle_sched_class(linux-3.10)，直到选择合适的运行进程  
- 可以看出：  
 - 只要存在实时进程，总是优于普通进程被调度运行  
 - 多个调度器同时存在，依次运行，直到选出下一个可运行进程  
  
  
## 系统调用  
**系统调用号**  
- 每个系统调用被赋予一个系统调用号。  
- 用户空间进程执行系统调用时，通过调用号进行调用，不会提及系统调用的名称。  
- 系统调用号在程序编译时指定。  
系统调用开销：  
- 上下文切换时间  
- 系统调用本身执行时间  
  
### 系统调用处理程序  
系统调用方式：  
- 系统调用：应用程序告诉内核自己需要执行一个系统调用，希望切换到内核态  
- 通知内核的机制是靠软中断实现：引发一个异常来促使系统切换到内核态去执行异常处理程序（即系统调用处理程序）  
- x86系统上预定义的软中断号是128，通过int $0x80指令触发该软中断（128号异常处理程序：系统调用处理程序system_call()）  
  
- 指定恰当的系统调用  
-- 陷入内核前，用户空间把相应的系统调用号放入eax中  
-- system_call()一旦运行，就可以从eax中得到数据  
- 参数传递：  
-- ebx, ecx, edx, esi和edi按照顺序存放前5个参数  
-- 超过5个时，用一个单独的寄存器存放指向所有这些参数的用户空间地址的指针  
- 返回值  
-- 存放在eax寄存器  
  
### 系统调用上下文  
内核在执行系统调用的时候处于进程上下文。current指针指向当前任务，即引发系统调用的那个进程。  
问题：多个CPU同时运行多个进程，此时如何设置current指针？系统中有多少个current指针？  
  
在进程上下文中（执行系统调用，或显示调用schedule()），内核可以休眠，并且可以被抢占。  
  
## 中断和中断处理  
硬件（外围设备）速度跟CPU不匹配。  
硬件完成后如何通知CPU：  
- CPU主动轮询  
- 硬件主动通知。硬件向内核发出信号  
  
中断控制器：  
- 接收硬件中断通知  
- 向CPU发送一个电信号  
  
每个中断都通过一个唯一的数字标志。通常被称为中断请求（IRQ）线。  
  
异常:  
- 处理器本身产生的同步中断  
-- 如处理器执行到由于编程失误而导致错误指令（0除）  
-- 缺页  
-- 必须依靠内核来处理  
  
### 中断处理程序  
每个中断都有一个对应的中断处理程序。  
- 如：系统时钟中断处理程序  
  
中断处理程序与其他内核函数的区别：  
- 中断处理程序是被内核调用来响应中断的  
- 运行于中断上下文中  
- 中断上下文被称作原子上下文，该上下文中的执行代码不可阻塞。  
  
### 上半部与下半部的对比  
中断需要快速完成，但中断处理程序需要完成大量工作（如网络中断：网络数据包拷贝，处理，交给合适的协议栈）。因此把中断处理切为两个部分。  
上半部：拷贝网络数据  
下半部：处理和操作数据  
  
中断处理是上半部 -- 接收到一个中断，立即开始执行，只做有严格时限的工作，如对中断进行应答或复位硬件。能够被允许稍后完成的工作会推迟到下半部。此后，在合适的时机，下半部会被开中断执行。  
  
中断处理程序无须可重入  
当一个给定的中断处理程序正在执行时，相应中断线在所有处理器上都会被屏蔽掉。  
  
### 中断处理程序不能睡眠  
睡眠和调度等价。中断运行在中断上下文中，没有调度实体，无法被调度。即不能睡眠的关键是因为上下文。  
操作系统以进程调度为单位，进程的运行在进程的上下文中，以进程描述符作为管理的数据结构。进程可以睡眠的原因是操作系统可以切换不同进程的上下文，进行调度操作，这些操作都以进程描述符为支持。  
中断运行在中断上下文，没有一个所谓的中断描述符来描述它，它不是操作系统调度的单位。一旦在中断上下文中睡眠，首先无法切换上下文（因为没有中断描述符，当前上下文的状态得不到保存），其次，没有人来唤醒它，因为它不是操作系统的调度单位。  
此外，中断的发生是非常非常频繁的，在一个中断睡眠期间，其它中断发生并睡眠了，那很容易就造成中断栈溢出导致系统崩溃。  
  
## 同步  
内核并发执行的原因:  
- 中断  
- 软中断和tasklet  
- 内核抢占  
- 睡眠  
- 对称多处理  
  
**同步方法**  
同步方法包含原子操作，自旋锁，互斥锁，信号量，条件变量等方法。下面分别介绍其实现原理。  
  
### 原子操作  
- 原子操作是其它同步方法的基石。  
- 原子操作依靠L3 cache MESI协议实现  
-- https://zhuanlan.zhihu.com/p/33445834  
-- http://www.wowotech.net/kernel_synchronization/445.html  
  
内核提供两组原子操作接口：  
- 一组针对整数进行操作  
- 一组针对单独的位进行操作  
缺少原子操作指令的体系结构，提供了单步执行锁内存总线的指令。  
  
原子性：32位或64位  
  
### 自旋锁  
一个变量（超小临界区）的操作可以使用自旋锁。  
自旋锁vs中断处理程序:  
- 自旋锁可以使用在中断处理程序中。  
- 中断处理程序使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（当前处理器上的中断请求），否则其它中断程序会打断持有锁的内核代码（当前中断），导致死锁  
  
自旋锁可依赖原子变量实现:  
- 循环查询原子变量的值  
- 很多版本的实现会在循环查询一定数量后主动让出CPU  
  
### 互斥锁  
互斥锁的实现原理。2.6版本  
struct mutex {  
  atomic_t  count;  // 引用计数器，初始值为1：锁可以使用；小于等于0：该锁已被获取，需要等待。  
  spinlock_t wait_lock;  // 自旋锁类型，保证多cpu下，对等待队列访问是安全的。  
  struct list_head wait_list;  // 等待队列，如果该锁被获取，任务将挂在此队列上，等待调度。  
};  
  
**互斥锁实现原理:**  
- 加锁时原子变量count减1，若减1后为0，锁获取成功；若减1后为负数，锁已经被占用  
- 锁等待：  
-- 把线程挂入锁等待队列mutex.wait_list  
-- 执行schedule()把线程从运行队列移入等待队列  
-- 优化：线程切换开销较大，可以尝试先自旋一定次数后若仍未能获取到锁再放入等待队列  
- 锁唤醒  
-- count加1，若小于等于0，说明有锁等待线程  
-- 把线程从mutex.wait_list中取出  
-- 执行wake_up()把线程从等待队列移入运行队列  
  
### 信号量  
信号量的实现原理类似互斥锁，只是count的初始值可设置为大于等于1.  
  
### 条件变量  
条件变量机制是将全局标记、线程信号、线程休眠、加锁解锁结合在了一起。主要内容包含在pthread_cond_wait函数内，包括：  
- 检测条件变量，为真从函数返回，继续执行下一步，为假，则：  
-- 设置线程等待条件（变量变为真的）信号  
-- 让线程进入休眠状态  
-- 释放互斥锁  
- 到条件（变量变为真的）信号后，则：  
-- 唤醒休眠的线程  
-- 重新获取互斥锁  
-- 重新检测条件变量，为假，则继续执行步骤1，为真则从函数内返回。  
  
pthread_cond_signal函数的主要工作是负责修改条件变量的值和发送信号提醒等待线程条件变量变成了真值。  
  
**互斥锁保护的是两种资源：**  
- 条件变量和线程需要访问的全局资源。  
- 因为线程在pthread_cond_wait函数内被唤醒后，重新获取了互斥锁，然后才能检测条件变量，条件变量为真，从函数返回后，并没有申请获取其他的互斥锁，就继续对全局资源访问，并在完成访问后只释放了一次互斥锁。  
  
### 互斥锁、信号量和条件变量的区别  
本质上都是生产者+消费者模型。互斥锁是谁消费谁负责再生产，信号量生产者和消费者可以是分开的，条件变量类似信号量，区别在于其一次生产支持唤醒所有的消费者(pthread_cond_broadcast)  
  
## 定时器和时间管理  
系统定时器：可编程芯片，固定频率产生中断。  
对应的中断处理程序负责更新时间。  
  
**时钟节拍：**  
- Linux早期版本时钟频率为100HZ/s  
- 2.5版本后提高到1000HZ/s  
  
时钟中断处理程序负责建设当前进程的时间片计数。  
低HZ存在的问题，如当前进程时间片为2ms，时钟频率100HZ，则当前进程比预期晚8ms被调度出去。  
高HZ会增加系统负载。现代计算机系统设置1000HZ比较合适。  
  
实时时钟：用来存放系统时间的设备  
  
### 时钟中断处理程序  
jiffies_64记录系统启动后的时钟中断计数。  
**执行时钟例程**  
- 给jiffies_64变量增加1  
- 更新资源消耗的统计值：如当前进程所消耗的系统时间和用户时间  
- 执行到期的动态定时器  
- 执行scheduler_tick()函数  
- 更新墙上时间，存放在xtime变量  
- 计数平均负载值  
  
### 实际时间  
当前实际时间（墙上时间）保存在xtime中。  
  
### 定时器  
定时器超时后调用指定的函数并自行撤销，并不周期复用，因此称为动态定时器。  
如中断下半部使用定时器推迟执行。  
  
**定时器使用：**  
- 指定超时节拍，jiffies + delay  
- 指定执行函数，function  
- 定时器保存在链表中  
- 时钟中断处理程序会执行超时的定时器  
  
**定时器的管理：**  
- 背景  
-- 时钟中断处理程序遍历整个定时器链表不明智  
-- 按超时时间排序效率低  
- 分成5组：  
-- 把定时器按超时时间放入对应对应的组  
  
**延迟执行策略：**  
- 忙等待。等待的事件执行时间很短，忙等待不主动让出CPU。会占用本进程的时间片  
- schedule_timeout()  
-- 指定延迟执行的任务睡眠到指定的延迟时间后再重新运行  
